{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2c7d1a-63d1-4928-95f8-0bea1a89e1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phi-4-reasoning Model Deployment and Inference\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Deploy the microsoft/Phi-4-reasoning model using MLflow extensions\n",
    "2. Set up the model for serving\n",
    "3. Perform inference on both text and image inputs\n",
    "4. Process images in batch using Spark\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks workspace with appropriate permissions\n",
    "- Access to Hugging Face models\n",
    "- Sufficient GPU resources for model deployment\n",
    "\n",
    "## Cluster Configuration\n",
    "- Runtime: 16.3 ML (includes Apache Spark 3.5.2, GPU, Scala 2.12)\n",
    "- Node Type: Standard_NC40ads_H100_v5 [H100] (beta)\n",
    "- 320 GB Memory\n",
    "- 1 GPU\n",
    "- 40 Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59d4573-57ad-440e-8813-3964fb96c075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "Installing necessary packages including:\n",
    "- OpenAI client\n",
    "- VLLM for efficient model serving\n",
    "- MLflow extensions for deployment\n",
    "- Transformers library with Qwen VL support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede3f7b2-1fb6-4466-aa89-71aea2c2c7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PIP_REQUIREMENTS = (\n",
    "    \"openai vllm==0.8.5.post1 optree \"\n",
    "    \"git+https://github.com/huggingface/transformers accelerate  \"\n",
    "    \"mlflow==2.19.0 \"\n",
    "    \"mlflow-extensions \"\n",
    "    \"qwen-vl-utils\"\n",
    ")\n",
    "\n",
    "%pip install {PIP_REQUIREMENTS}\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5f8118-ad1e-4666-99fd-26cb5b3957a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the necessary configuration parameters for model deployment:\n",
    "- Catalog and schema for model registration\n",
    "- Model and endpoint names\n",
    "- Environment variables for VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3b0c68-90d7-4c22-903a-974d79ea8c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CATALOG = \"...\"\n",
    "SCHEMA = \"...\"\n",
    "MODEL_NAME = \"...\"\n",
    "ENDPOINT_NAME = \"...\"\n",
    "\n",
    "# Set environment variables for VLLM\n",
    "import os\n",
    "# os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "# os.environ['VLLM_USE_V1'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7d351e-7f85-4ee3-b1ee-5dc854024641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PIP_REQUIREMENTS = (\n",
    "    \"openai vllm==0.8.5.post1 optree \"\n",
    "    \"git+https://github.com/huggingface/transformers accelerate  \"\n",
    "    \"mlflow==2.19.0 \"\n",
    "    \"mlflow-extensions \"\n",
    "    \"qwen-vl-utils\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fb9d41-321a-4d76-935e-abd07ae66642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from mlflow_extensions.serving.engines import VLLMEngineProcess\n",
    "from mlflow_extensions.serving.engines.vllm_engine import VLLMEngineConfig\n",
    "from mlflow_extensions.databricks.deploy.ez_deploy import EzDeployConfig, ServingConfig, EzDeployVllmOpenCompat\n",
    "\n",
    "# Replace 'your_huggingface_token' with your actual Hugging Face token\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc03d71-d0c2-497c-8d31-341357c575a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the deployer with VLLM OpenAI compatibility layer\n",
    "deployer = EzDeployVllmOpenCompat(\n",
    "  config= EzDeployConfig(\n",
    "    # Specify the model name/path from Hugging Face\n",
    "    name=\"microsoft/Phi-4-reasoning\",\n",
    "    # Use VLLM engine process for serving\n",
    "    engine_proc=VLLMEngineProcess,\n",
    "    engine_config=VLLMEngineConfig(\n",
    "          # Model identifier on Hugging Face\n",
    "          model=\"microsoft/Phi-4-reasoning\",\n",
    "          # Maximum sequence length for input\n",
    "          max_model_len = 30000,\n",
    "          # Maximum number of images/videos that can be processed\n",
    "          # VLLM specific configuration flags\n",
    "          vllm_command_flags={\n",
    "            # GPU memory utilization target (98%)\n",
    "            \"--gpu-memory-utilization\": .95,\n",
    "            # Disable caching of preprocessed multimedia\n",
    "            \"--disable-mm-preprocessor-cache\" : None,\n",
    "            # Enable automatic tool selection\n",
    "            \"--enable-auto-tool-choice\": None,\n",
    "            # Use hermes parser for tool calls\n",
    "            \"--tool-call-parser\" : \"hermes\",\n",
    "          },\n",
    "),\n",
    "  serving_config=ServingConfig(\n",
    "      # Minimum memory required for model serving (in GB)\n",
    "      # Includes model weights, KV cache, overhead and intermediate states\n",
    "      minimum_memory_in_gb=60,\n",
    "  ),\n",
    "  # Use pip requirements defined earlier\n",
    "  pip_config_override = PIP_REQUIREMENTS.split(\" \")\n",
    "),\n",
    "  # Register model with fully qualified name in Unity Catalog\n",
    "  registered_model_name=f\"{CATALOG}.{SCHEMA}.{MODEL_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d81401ea-c356-4a55-a2b9-30feba4a5266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Registration and Deployment\n",
    "\n",
    "Download and register the model in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66e9e7a-d976-4152-83ef-cde2b8a480d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download and register the model\n",
    "deployer.artifacts = deployer._config.download_artifacts(local_dir=\"/tmp/\") #this can be volume location as well\n",
    "deployer._downloaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9342a76f-fe66-43bd-b8e1-521952aed95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deployer.register() # Ignore error as this will fail in serverless as there are no GPU's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "555643a8-eeb2-4297-8457-8c93a8f36304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Below is the code to deploy the endpoint to model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e24f7b8-9bbf-42fc-9881-319912f8f079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Deployment to Serving Endpoint\n",
    "\n",
    "Deploy the registered model to a serving endpoint. This will:\n",
    "1. Create a new serving endpoint with the specified name\n",
    "2. Load the model into memory\n",
    "3. Make it available for inference requests\n",
    "\n",
    "Note: `scale_to_zero=False` means the endpoint will maintain at least one instance running,\n",
    "which helps reduce cold start times but may incur higher costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c754ff3-e1ad-490c-8f0b-da93bbc3d7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deployer.deploy(ENDPOINT_NAME, scale_to_zero=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d658428d-81f2-4cfa-a806-9b0681d36a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process Management\n",
    "\n",
    "### Restarting Model Processes\n",
    "\n",
    "Sometimes you may need to restart the model processes, for example:\n",
    "- After making configuration changes\n",
    "- If the model becomes unresponsive\n",
    "- To free up GPU memory\n",
    "\n",
    "The following code will:\n",
    "1. Kill any existing VLLM processes\n",
    "2. Kill any Ray processes (used for distributed computing)\n",
    "3. Kill any multiprocessing processes\n",
    "\n",
    "Run this cell whenever you need to restart the model processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969f0357-d103-40cc-a41b-c12550acec76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow_extensions.testing.helper import kill_processes_containing\n",
    "\n",
    "# Kill existing processes to free up resources\n",
    "kill_processes_containing(\"vllm\")  # Kill VLLM model serving processes\n",
    "kill_processes_containing(\"ray\")   # Kill Ray distributed computing processes\n",
    "kill_processes_containing(\"from multiprocessing\")  # Kill any multiprocessing processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eb13470-a08c-4040-8949-3f338159b065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Serving Setup\n",
    "\n",
    "Initialize the model for serving and set up the client for inference.\n",
    "This section will:\n",
    "1. Set up MLflow registry URI\n",
    "2. Fetch the latest model version\n",
    "3. Load the model for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a83f1fe-41e4-4035-b709-4e4e7c22650d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set up MLflow registry\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get the latest model version\n",
    "model_name = f\"{CATALOG}.{SCHEMA}.{MODEL_NAME}\"\n",
    "latest_version = None\n",
    "\n",
    "# Iterate through versions to find the latest one\n",
    "for i in range(1, 10):\n",
    "    try:\n",
    "        client.get_model_version(model_name, i)\n",
    "    except:\n",
    "        latest_version = i - 1\n",
    "        break\n",
    "\n",
    "if latest_version is None:\n",
    "    raise Exception(\"Could not determine latest model version\")\n",
    "\n",
    "print(f\"Using latest model version: {latest_version}\")\n",
    "\n",
    "# Load the registered model\n",
    "model_uri = f\"models:/{model_name}/{latest_version}\"\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_uri)\n",
    "base_url = str(pyfunc_model.unwrap_python_model()._engine._server_http_client.base_url)\n",
    "\n",
    "print(\"Model serving base URL:\", base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3886e4b4-6b85-46b6-a354-b60906be0e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference Examples\n",
    "\n",
    "Demonstrate model inference capabilities with different types of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a0336c-3937-4b8a-a245-f8b7e6c1c290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Text-only Inference\n",
    "\n",
    "Basic text completion example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d70d0a-f504-42aa-954e-563563592db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "serving_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello! how is the weather today ?\"\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_tokens\": 10000,\n",
    "}\n",
    "\n",
    "response = pyfunc_model.predict(serving_payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad21d29-c115-41e5-b0bc-9265892e68f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Image Analysis\n",
    "\n",
    "Example of analyzing an image with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946b7b19-52fb-4230-b7a1-b098d87d9a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=f\"{base_url}/v1\",\n",
    "    api_key=\"DUMMY\"\n",
    ")\n",
    "\n",
    "# Load and display test image\n",
    "image_url = \"https://www.arsenal.com/sites/default/files/styles/large_16x9/public/images/saka-celeb-bayern.png?h=3c8f2bed&auto=webp&itok=Twjeu8tug\"\n",
    "with urllib.request.urlopen(image_url) as url:\n",
    "    img = Image.open(BytesIO(url.read()))\n",
    "display(img)\n",
    "\n",
    "# Perform image analysis\n",
    "response = client.chat.completions.create(\n",
    "    model=\"default\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Which football team does this player belong to?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url,\n",
    "                        \"detail\": \"high\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9121ba7-d62a-4d13-a1d7-1d174ab4cea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, regexp_replace\n",
    "import pandas as pd\n",
    "from io import BytesIO \n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize OpenAI client with local endpoint\n",
    "client = OpenAI(\n",
    "        base_url=f\"{'http://0.0.0.0:9989'}/v1\",\n",
    "        api_key=\"DUMMY\")\n",
    "\n",
    "# Path to local image file\n",
    "img_path = \"/Volumes/samantha_wise/gsk_vlm_poc/images/Goldfish-2-e1724099193229.png\" # change here\n",
    "\n",
    "# Read the local image file into bytes and convert to base64\n",
    "# Steps:\n",
    "# 1. Open image file in binary mode and read bytes\n",
    "# 2. Create BytesIO buffer to hold the bytes in memory\n",
    "# 3. Encode bytes to base64 string for API transmission\n",
    "# 4. Decode to UTF-8 string since API expects text\n",
    "# This converts the binary image data into a text format that can be sent in the API request\n",
    "with open(img_path, 'rb') as f:\n",
    "    # Create BytesIO object from image bytes\n",
    "    image_file = BytesIO(f.read())\n",
    "    # Convert bytes to base64 string for API\n",
    "    # Base64 encoding ensures binary image data can be transmitted as text\n",
    "    image_base64 = base64.b64encode(image_file.getvalue()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215df940-5206-4482-bc4f-582ac75cad67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make API call with base64 encoded image\n",
    "# The image is passed as a data URL in the format:\n",
    "# data:image/png;base64,<base64_string>\n",
    "# This format allows embedding binary image data directly in the request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"default\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                    {\n",
    "                         \"type\": \"text\", \n",
    "                         \"text\": \"\"\"OCR and give details and look at everything and do not hallucinate and think carefully \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{image_base64}\",  # Pass base64 image as data URL\n",
    "                            'detail': 'high'\n",
    "                        }\n",
    "                    }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens = 5000,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.95,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n",
    "#\"OCR this image, provide the context of the image and return the output in table format with 4 columns namely PRODUCT_TYPE, PRODUCT_TEXT, PRODUCT_NUMBER and LEGEND\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7309c4f-afe1-462f-b21a-fd26359b225a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Load files from Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474436ca-cb97-4d5a-b928-b7f9cccdce21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "    .option(\"pathGlobfilter\", f\"*.jpg\")\n",
    "    .load(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894bbc0f-ab35-40d2-b18b-1c86cb28d569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_NAME = \"table_name\" # change here\n",
    "\n",
    "df_img = spark.table(f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\")\n",
    "display(df_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46309d98-6d99-4ded-8371-6aa88ceb7629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, regexp_replace\n",
    "import pandas as pd\n",
    "\n",
    "prompt = \"This image contains a human. Your task is to tell me what this person is doing and try to identify who they are.\" \n",
    "\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def classify_img(images: pd.Series) -> pd.Series:\n",
    "    def classify_one_image(img):\n",
    "        client = OpenAI(\n",
    "            base_url=f\"{base_url}/v1\",\n",
    "            api_key=\"DUMMY\"\n",
    "        )\n",
    "\n",
    "        image_file = BytesIO(img)\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"default\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    return pd.Series([classify_one_image(img) for img in images])\n",
    "\n",
    "# Example usage with Spark DataFrame\n",
    "df_inference = df_img.repartition(4).withColumn(\"vLLM_predict\", classify_img(\"content\"))\n",
    "display(df_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08458ff-1c75-4fc9-8ef1-227666b8893f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up processes when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3342200d-2170-4d41-9e49-c0d6a601254a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow_extensions.testing.helper import kill_processes_containing\n",
    "\n",
    "kill_processes_containing(\"vllm\")\n",
    "kill_processes_containing(\"ray\")\n",
    "kill_processes_containing(\"from multiprocessing\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "vLMM Phi4 Template",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
